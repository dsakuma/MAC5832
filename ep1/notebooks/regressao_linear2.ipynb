{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão linear 2:  gradiente descendente\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aprendizado e otimização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dada uma função *target* desconhecida $f:\\mathbb{R}^{d} \\rightarrow \\mathbb{R}$ e uma hipótese $h_{\\mathbf{w}}:\\mathbb{R}^{d} \\rightarrow \\mathbb{R}$ o erro de $h_{\\mathbf{w}}$ é definido por:\n",
    "\n",
    "\\begin{equation}\n",
    "E_{out}(h_{\\mathbf{w}}) = \\mathbb{E}_{\\mathbf{x}\\sim p_{data}}L(h(\\mathbf{x}; \\mathbf{w}), \\; f(\\mathbf{x}))\n",
    "\\end{equation}\n",
    "\n",
    "em que $p_{data}$ é a distribuição geradora dos dados, $L$  é alguma função de perda (por exemplo, o quadrado da diferença) e $h(\\mathbf{x}; \\mathbf{w}) = h_{\\mathbf{w}}(\\mathbf{x})$. Se tivéssemos acesso a $p_{data}$ poderíamos calcular a função acima para qualquer $h_{\\mathbf{w}}$ e escolher aquele com erro mínimo.\n",
    "\n",
    "Como não temos acesso a $p_{data}$, define-se\n",
    "\n",
    "\\begin{equation}\n",
    "E_{in}(h_{\\mathbf{w}}) = J(\\mathbf{w}) = \\frac{1}{N}\\sum_{i=1}^{N} L(h(\\mathbf{x}_{i}; \\mathbf{w}), \\; y_{i})\n",
    "\\end{equation}\n",
    "\n",
    "em que $N$ é o tamanho do dataset de treinamento e $y_{i} = f(\\mathbf{x}_{i})$.\n",
    "\n",
    "Como visto em aula, uma relação entre $E_{in}$ e $E_{out}$ pode ser estabelecida pela **Inequação de Hoeffding**.\n",
    "\n",
    "Aqui estamos interessados em encontrar $h_{\\mathbf{w}}$ com erro $J(\\mathbf{w})$ mínimo. Isto corresponde a determinar o ponto mínimo da função $J(\\mathbf{w})$. Por isso, vamos nos concentrar apenas em uma técnica de otimização para minimizar $E_{in}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradiente ascendente e gradiente descendente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dados $\\mathbf{w}, \\mathbf{u} \\in \\mathbb{R}^{d}$ e $J:\\mathbb{R}^{d} \\rightarrow \\mathbb{R}$ tal que $||\\mathbf{u}||_{2} = 1$ a taxa de variação de $J$ no ponto $\\mathbf{w}$ em direção a $\\mathbf{u}$ é chamada de **derivada direcional**, $D_{\\mathbf{u}}J(\\mathbf{w})$. Definindo $g(h) = J(\\mathbf{w} + h\\mathbf{u})$, podemos usar a regra da cadeia e a definição de produto escalar para mostrar que  \n",
    "\n",
    "\\begin{equation}\n",
    "D_{\\mathbf{u}}J(\\mathbf{w}) \\;\\;=\\;\\; \\left. \\frac{dg}{dh} \\right|_{h=0} \\;\\;=\\;\\; \\mathbf{u}^{T}\\nabla_{\\mathbf{w}}J(\\mathbf{w})\n",
    "    \\;\\;=\\;\\; ||\\mathbf{u}||_{2}||\\nabla_{\\mathbf{w}}J(\\mathbf{w})||_{2}cos\\theta \\;\\;=\\;\\; ||\\nabla_{\\mathbf{w}}J(\\mathbf{w})||_{2}cos\\theta\n",
    "\\end{equation}\n",
    "\n",
    "em que $\\theta$ é o ângulo entre $\\mathbf{u}$ e $\\nabla_{\\mathbf{w}}J(\\mathbf{w})$. Assim, pensando em $D_{\\mathbf{u}}J(\\mathbf{w})$ em termos do vetor $\\mathbf{u}$ temos que: $D_{\\mathbf{u}}J(\\mathbf{w})$ tem o valor máximo quando $\\mathbf{u}$ tem a mesma direção que $\\nabla_{\\mathbf{w}}J(\\mathbf{w})$ ($\\theta=0$); similarmente $D_{\\mathbf{u}}J(\\mathbf{w})$ tem o valor mínimo quando $\\mathbf{u}$ tem a direção oposta a $\\nabla_{\\mathbf{w}}J(\\mathbf{w})$ ($\\theta=\\pi$).\n",
    "\n",
    "Desse modo podemos maximizar $J$ alterando $\\mathbf{w}$ na direção de $\\nabla_{\\mathbf{w}}J(\\mathbf{w})$ e podemos minimizar $J$ alterando $\\mathbf{w}$ na direção de $-\\nabla_{\\mathbf{w}}J(\\mathbf{w})$. Isso permite dois métodos bem simples de otimização:\n",
    "\n",
    "**Gradiente Ascendente**\n",
    "\n",
    "- $\\mathbf{w}(0) = \\mathbf{w}$\n",
    "- for $t = 0, 1, 2, \\dots$ do\n",
    "    * $\\mathbf{w}(t+1) = \\mathbf{w}(t) + \\eta \\nabla_{\\mathbf{w}(t)}J(\\mathbf{w}(t))$ \n",
    "\n",
    "    \n",
    "\n",
    "**Gradiente Descendente**\n",
    "\n",
    "- $\\mathbf{w}(0) = \\mathbf{w}$\n",
    "- for $t = 0, 1, 2, \\dots$ do\n",
    "    * $\\mathbf{w}(t+1) = \\mathbf{w}(t) - \\eta \\nabla_{\\mathbf{w}(t)}J(\\mathbf{w}(t))$ \n",
    "\n",
    "Em ambos os casos o parâmetro $\\eta \\in \\mathbb{R}_{\\geq}$ é chamado de **taxa de aprendizado** (*learning rate*); ele pondera o tamanho de cada atualização."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####   A seguir são apresentados uma sequência de exercícios que ilustram diferentes aspectos práticos na implementação do algoritmo *gradient descent*  \n",
    "Iremos considerar o problema de regressão linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all imports\n",
    "import numpy as np\n",
    "import time\n",
    "from util import r_squared, randomize_in_place, get_housing_prices_data\n",
    "from plots import simple_step_plot, plot_points_regression, plot_cost_function_curve\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Vamos usar o mesmo dataset de antes, mas agora vamos dividir os dados em treinamento, validação e teste.\n",
    "Essa divisão é comumente realizada na prática: os dados de treinamento são usados na otimização da função custo $J$, os dados de validação são usados para aferir a qualidade da otimização, e os dados de teste são usados para aferir a qualidade da predição."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dividindo os dados em treinamento, validação e teste\n",
      "\n",
      "train_X shape = (250, 1)\n",
      "\n",
      "train_y shape = (250, 1)\n",
      "\n",
      "valid_X shape = (50, 1)\n",
      "\n",
      "valid_y shape = (50, 1)\n",
      "\n",
      "test_X shape = (50, 1)\n",
      "\n",
      "test_y shape = (50, 1)\n"
     ]
    }
   ],
   "source": [
    "X, y = get_housing_prices_data(N=350, verbose=False)\n",
    "randomize_in_place(X, y)\n",
    "\n",
    "train_X = X[0:250]\n",
    "train_y = y[0:250]\n",
    "valid_X = X[250:300]\n",
    "valid_y = y[250:300]\n",
    "test_X = X[300:]\n",
    "test_y = y[300:]\n",
    "\n",
    "print(\"\\nDividindo os dados em treinamento, validação e teste\")\n",
    "print(\"\\ntrain_X shape = {}\".format(train_X.shape))\n",
    "print(\"\\ntrain_y shape = {}\".format(train_y.shape))\n",
    "print(\"\\nvalid_X shape = {}\".format(valid_X.shape))\n",
    "print(\"\\nvalid_y shape = {}\".format(valid_y.shape))\n",
    "print(\"\\ntest_X shape = {}\".format(test_X.shape))\n",
    "print(\"\\ntest_y shape = {}\".format(test_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f105c04aa90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_points_regression(train_X,\n",
    "                       train_y,\n",
    "                       title='Training data',\n",
    "                       xlabel=\"m\\u00b2\",\n",
    "                       ylabel='$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para minimizar a função de custo vamos ter que colocar os dados em uma outra escala\n",
    "\n",
    "Um modo de fazer isso é o chamado [standard/z score](https://en.wikipedia.org/wiki/Standard_score).\n",
    "Aplicamos a seguinte transformação: \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{X}^{\\top}_{i} \\leftarrow \\frac{\\mathbf{X}^{\\top}_{i} - \\mu_{i}}{\\sigma_{i}}\n",
    "\\end{equation}\n",
    "\n",
    "em que $\\mathbf{X}^{T}_{i} \\in \\mathbb{R}^{N}$ ($i = 1, \\dots, d$) é um vetor de features da design matrix $\\mathbf{X}$, $\\mu_{i}$ é a média de tal vetor, e $\\sigma_{i}$ seu desvio padrão.\n",
    "\n",
    "A importância de se fazer essa transformação é discutida mais ao final deste notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  **Exercício 1)** \n",
    "Use a biblioteca numpy para implementar a função que altera os dados conforme a equação acima (essa função deve funcionar para uma design matrix $\\mathbf{X}$ com um número arbitrário de features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 2.],\n",
       "       [3., 4., 5.],\n",
       "       [6., 7., 8.]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = np.arange(3.0)\n",
    "x1 = np.arange(9.0).reshape((3, 3))\n",
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(X):\n",
    "    \"\"\"\n",
    "    Returns standardized version of the ndarray 'X'.\n",
    "\n",
    "    :param X: input array\n",
    "    :type X: np.ndarray(shape=(N, d))\n",
    "    :return: standardized array\n",
    "    :rtype: np.ndarray(shape=(N, d))\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE:\n",
    "    mean = X.mean(axis=0)\n",
    "    std_dev = X.std(axis=0)\n",
    "    X_out = (X-mean)/std_dev\n",
    "    # END YOUR CODE\n",
    "\n",
    "    return X_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Teste exercício 1)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    toy_X = np.array([[1100.3, 2.4, 34.34],\n",
    "                      [2300.3, 1.4, 442.23]])\n",
    "    toy_y = np.array([[1000.2], [2000.5]])\n",
    "    toy_X_norm = standardize(toy_X)\n",
    "    toy_y_norm = standardize(toy_y)\n",
    "    xmean, xstd = np.mean(toy_X_norm), np.std(toy_X_norm)\n",
    "    ymean, ystd = np.mean(toy_y_norm), np.std(toy_y_norm)\n",
    "    assert -1 <= xmean < 0\n",
    "    assert 0 <= ymean < 1\n",
    "    assert 0.9 <= xstd <= 1\n",
    "    assert 0.9 <= ystd <= 1\n",
    "\n",
    "except NotImplementedError:\n",
    "    print(\"Falta fazer!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados originais\n",
      "\n",
      "X:\n",
      "mean 636.5652465820312, std 323.76, max 1200.0, min 93.1805191040039\n",
      "\n",
      "y:\n",
      "mean 43664.78515625, std 16499.49, max 79307.90625, min 10361.91796875\n",
      "\n",
      "Dados normalizados\n",
      "\n",
      "X:\n",
      "mean 7.915496524901755e-08, std 1.00, max 1.740271806716919, min -1.6783435344696045\n",
      "\n",
      "y:\n",
      "mean -2.6702881683604573e-08, std 1.00, max 2.1602554321289062, min -2.0184173583984375\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1059fbb6a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    train_X_norm = standardize(train_X)\n",
    "    train_y_norm = standardize(train_y)\n",
    "\n",
    "\n",
    "    xmean, xstd = np.mean(train_X), np.std(train_X)\n",
    "    xmax, xmin = np.max(train_X), np.min(train_X)\n",
    "    ymean, ystd = np.mean(train_y), np.std(train_y)\n",
    "    ymax, ymin = np.max(train_y), np.min(train_y)\n",
    "\n",
    "    print(\"Dados originais\\n\")\n",
    "    print(\"X:\\nmean {}, std {:.2f}, max {}, min {}\".format(xmean,\n",
    "                                                           xstd,\n",
    "                                                           xmax,\n",
    "                                                           xmin))\n",
    "    print(\"\\ny:\\nmean {}, std {:.2f}, max {}, min {}\\n\".format(ymean,\n",
    "                                                             ystd,\n",
    "                                                             ymax,\n",
    "                                                             ymin))\n",
    "\n",
    "\n",
    "    xmean, xstd = np.mean(train_X_norm), np.std(train_X_norm)\n",
    "    xmax, xmin = np.max(train_X_norm), np.min(train_X_norm)\n",
    "    ymean, ystd = np.mean(train_y_norm), np.std(train_y_norm)\n",
    "    ymax, ymin = np.max(train_y_norm), np.min(train_y_norm)\n",
    "\n",
    "    print(\"Dados normalizados\\n\")\n",
    "    print(\"X:\\nmean {}, std {:.2f}, max {}, min {}\".format(xmean,\n",
    "                                                           xstd,\n",
    "                                                           xmax,\n",
    "                                                           xmin))\n",
    "    print(\"\\ny:\\nmean {}, std {:.2f}, max {}, min {}\\n\".format(ymean,\n",
    "                                                             ystd,\n",
    "                                                             ymax,\n",
    "                                                             ymin))\n",
    "    plot_points_regression(train_X_norm,\n",
    "                           train_y_norm,\n",
    "                           title='Training data (normalized)',\n",
    "                           xlabel=\"m\\u00b2\",\n",
    "                           ylabel='$')\n",
    "\n",
    "except NotImplementedError:\n",
    "    print(\"Falta fazer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Adicionando uma componente com apenas 1s como uma nova feature\n",
    "Conforme já vimos, adicionar uma componente (coordenada artificial) constante 1 é conveniente. Isto é, em vez de $\\mathbf{x} \\in \\mathbb{R}^d$ é conveniente considerarmos $(1,\\mathbf{x}) \\in \\mathbb{R}^{d+1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature_ones(X):\n",
    "    \"\"\"\n",
    "    Returns the ndarray 'X' with the extra\n",
    "    feature column containing only 1s.\n",
    "\n",
    "    :param X: input array\n",
    "    :type X: np.ndarray(shape=(N, d))\n",
    "    :return: output array\n",
    "    :rtype: np.ndarray(shape=(N, d+1))\n",
    "    \"\"\"\n",
    "    return np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
    "\n",
    "try:\n",
    "    train_X_1 = add_feature_ones(train_X_norm)\n",
    "    print(\"\\ntrain_X shape = {}\".format(train_X_1.shape))\n",
    "\n",
    "except NameError:\n",
    "    print(\"Falta fazer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Criando a predição da regressão linear e plotando uma predição arbitrária"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_prediction(X, w):\n",
    "    \"\"\"\n",
    "    Calculates the linear regression prediction.\n",
    "\n",
    "    :param X: design matrix\n",
    "    :type X: np.ndarray(shape=(N, d))\n",
    "    :param w: weights\n",
    "    :type w: np.array(shape=(d, 1))\n",
    "    :return: prediction\n",
    "    :rtype: np.array(shape=(N, 1))\n",
    "    \"\"\"\n",
    "\n",
    "    return X.dot(w)\n",
    "\n",
    "try:\n",
    "    w = np.array([[1.2], [2.3]])\n",
    "    prediction = linear_regression_prediction(train_X_1, w)\n",
    "\n",
    "    plot_points_regression(train_X_norm,\n",
    "                           train_y_norm,\n",
    "                           title='Training data (normalized)',\n",
    "                           xlabel=\"m\\u00b2\",\n",
    "                           ylabel='$',\n",
    "                           prediction=prediction,\n",
    "                           legend=True)\n",
    "except NameError:\n",
    "    print(\"Falta fazer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computando a função de custo\n",
    "\n",
    "Usando o erro quadrárico médio, a função de custo $J(\\mathbf{w})$ para a tarefa de regressão linear pode ser escrita de dois modos. A forma iterativa:\n",
    "\n",
    "\\begin{equation}\n",
    "J(\\mathbf{w}) = \\frac{1}{N}\\sum_{i=1}^{N}(\\hat{y}_{i} - y_{i})^{2}\n",
    "\\end{equation}\n",
    "\n",
    "e a forma vetorial:\n",
    "\n",
    "\\begin{equation}\n",
    "J(\\mathbf{w}) = \\frac{1}{N}(\\mathbf{X}\\mathbf{w} - \\mathbf{y})^{T}(\\mathbf{X}\\mathbf{w} - \\mathbf{y})\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  **Exercício 2)**  \n",
    "Use a biblioteca numpy para implementar a função de custo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(X, y, w):\n",
    "    \"\"\"\n",
    "    Calculates  mean square error cost.\n",
    "\n",
    "    :param X: design matrix\n",
    "    :type X: np.ndarray(shape=(N, d))\n",
    "    :param y: regression targets\n",
    "    :type y: np.ndarray(shape=(N, 1))\n",
    "    :param w: weights\n",
    "    :type w: np.array(shape=(d, 1))\n",
    "    :return: cost\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE:\n",
    "    raise NotImplementedError\n",
    "    # END YOUR CODE\n",
    "\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Teste exercício 2)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    toy_w = np.array([[1], [1], [2]])\n",
    "    toy_X = np.array([[2, 3, 1],\n",
    "                      [5, 1, 2]])\n",
    "    toy_y = np.array([[1], [1]])\n",
    "    assert compute_cost(toy_X, toy_y, toy_w) == 58.5\n",
    "except NotImplementedError:\n",
    "    print(\"Falta fazer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos olhar a superficie de custo e ver onde se situa um valor $J(\\mathbf{w})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:    \n",
    "    initial_w = np.array([[15], [-35.3]])\n",
    "    initial_J = compute_cost(train_X_1, train_y_norm, initial_w)\n",
    "\n",
    "    plot_cost_function_curve(train_X_1,\n",
    "                             train_y_norm,\n",
    "                             compute_cost,\n",
    "                             title=\"Optimization landscape\",\n",
    "                             weights_list=[initial_w.flatten()],\n",
    "                             cost_list=[initial_J])\n",
    "except (NotImplementedError, NameError):\n",
    "    print(\"Falta fazer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculando os gradientes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É fácil calcular a derivada parcial de $J(\\mathbf{w})$ com relação a cada entrada $j$ de $\\mathbf{w}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial J(\\mathbf{w})}{\\partial \\mathbf{w}_{j}} = \\frac{2}{N}\\sum_{i=1}^{N} (\\hat{y}_i - y_i) \\mathbf{x}_{ij}\n",
    "\\end{equation}\n",
    "\n",
    "Lembre que o gradiente de $J(\\mathbf{w})$ com relação a $\\mathbf{w}$ é:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_{\\mathbf{w}}J(\\mathbf{w}) = \\begin{bmatrix}\\frac{\\partial J(\\mathbf{w})}{\\partial \\mathbf{w}_{1}} \\dots \\frac{\\partial J(\\mathbf{w})}{\\partial \\mathbf{w}_{m}} \\end{bmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  **Exercício 3)**  \n",
    "Use a biblioteca numpy para calcular $\\nabla_{\\mathbf{w}}J(\\mathbf{w})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_wgrad(X, y, w):\n",
    "    \"\"\"\n",
    "    Calculates gradient of J(w) with respect to w.\n",
    "\n",
    "    :param X: design matrix\n",
    "    :type X: np.ndarray(shape=(N, d))\n",
    "    :param y: regression targets\n",
    "    :type y: np.ndarray(shape=(N, 1))\n",
    "    :param w: weights\n",
    "    :type w: np.array(shape=(d, 1))\n",
    "    :return: gradient\n",
    "    :rtype: np.array(shape=(d, 1))\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE:\n",
    "    raise NotImplementedError\n",
    "    # END YOUR CODE\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Teste exercício 3)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_check(X, y, w, h=1e-4):\n",
    "    \"\"\"\n",
    "    Check gradients for linear regression.\n",
    "\n",
    "    :param X: design matrix\n",
    "    :type X: np.ndarray(shape=(N, d))\n",
    "    :param y: regression targets\n",
    "    :type y: np.ndarray(shape=(N, 1))\n",
    "    :param w: weights\n",
    "    :type w: np.array(shape=(d, 1))\n",
    "    :param h: small variation\n",
    "    :type h: float\n",
    "    :return: gradient test\n",
    "    :rtype: boolean\n",
    "    \"\"\"\n",
    "    Jw = compute_cost(X, y, w)\n",
    "    grad = compute_wgrad(X, y, w)\n",
    "    passing = True\n",
    "    d = w.shape[0]\n",
    "    for i in range(d):\n",
    "        w_plus_h = np.array(w, copy=True)\n",
    "        w_plus_h[i] = w_plus_h[i] + h\n",
    "        Jw_plus_h = compute_cost(X, y, w_plus_h)\n",
    "        w_minus_h = np.array(w, copy=True)\n",
    "        w_minus_h[i] = w_minus_h[i] - h\n",
    "        Jw_minus_h = compute_cost(X, y, w_minus_h)\n",
    "        numgrad_i = (Jw_plus_h - Jw_minus_h) / (2 * h)\n",
    "        reldiff = abs(numgrad_i - grad[i]) / max(1, abs(numgrad_i), abs(grad[i]))\n",
    "        if reldiff > 1e-5:\n",
    "            passing = False\n",
    "            msg = \"\"\"\n",
    "            Seu gradiente = {0}\n",
    "            Gradiente numérico = {1}\"\"\".format(grad[i], numgrad_i)\n",
    "            print(\"            \" + str(i) + \": \" + msg)\n",
    "            print(\"            Jw = {}\".format(Jw))\n",
    "            print(\"            Jw_plus_h = {}\".format(Jw_plus_h))\n",
    "            print(\"            Jw_minus_h = {}\\n\".format(Jw_minus_h))\n",
    "\n",
    "    if passing:\n",
    "        print(\"Gradiente passando!\")\n",
    "    \n",
    "    return passing \n",
    "\n",
    "try:\n",
    "    toy_w1 = np.array([[1.], [2.], [1.], [2.]])\n",
    "    toy_X1 = np.array([[2., 3., 1., 2.],\n",
    "                      [5., 1., 1., 2.]])\n",
    "    toy_y1 = np.array([[1.], [-1.]])\n",
    "    toy_w2 = np.array([[-100.22], [20002.1], [102.5]])\n",
    "    toy_X2 = np.array([[2111.3, -2223., 404.0],\n",
    "                      [5222., -22221., 3.3]])\n",
    "    toy_y2 = np.array([[122.], [221.]])\n",
    "    toy_w3 = np.array([[-10.22], [-3.1]])\n",
    "    toy_X3 = np.array([[1.3, -1.2],\n",
    "                      [2.2, -2.1],\n",
    "                      [-2.3, -5.5],\n",
    "                      [3.2, 8.1],\n",
    "                      [3.3, -1.1],\n",
    "                      [-3.4, -2.22],\n",
    "                      [2.23, -4.4],\n",
    "                      [5.2, -2.3]])\n",
    "    toy_y3 = np.array([[10.3],\n",
    "                       [23.3],\n",
    "                       [10.1],\n",
    "                       [-20.2],\n",
    "                       [-10.2],\n",
    "                       [20.2],\n",
    "                       [-14.4],\n",
    "                       [-30.3]])\n",
    "    \n",
    "    assert grad_check(toy_X1, toy_y1, toy_w1)\n",
    "    assert grad_check(toy_X2, toy_y2, toy_w2)\n",
    "    assert grad_check(toy_X3, toy_y3, toy_w3)\n",
    "\n",
    "except NotImplementedError:\n",
    "    print(\"Falta fazer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch gradient descent\n",
    "\n",
    "A versão mais simples do algoritmo *gradient descent* faz uso de todas as observações do dataset de treinamento (esse algoritmo também é conhecido como *batch gradient descent* ou *vanilla gradient descent*).\n",
    "\n",
    "**Batch gradient descent**\n",
    "\n",
    "- $\\mathbf{w}(0) = \\mathbf{w}$\n",
    "- for $t = 0, 1, 2, \\dots$ do\n",
    "    * Compute the gradient $\\nabla_{\\mathbf{w}(t)}J(\\mathbf{w}(t))$ \n",
    "    * Apply update : $\\mathbf{w}(t+1) = \\mathbf{w}(t) - \\eta \\nabla_{\\mathbf{w}(t)}J(\\mathbf{w}(t))$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  **Exercício 4)** \n",
    "Implemente o algoritmo batch gradient descent com a taxa de apreendizado fixa para a regressão linear. A função abaixo deve retornar três coisas: o vetor de pesos $\\mathbf{w}$, uma lista com cada peso obtido ao longo do treinamento, e uma lista com o custo de cada peso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_gradient_descent(X, y, w, learning_rate, num_iters):\n",
    "    \"\"\"\n",
    "     Performs batch gradient descent optimization.\n",
    "\n",
    "    :param X: design matrix\n",
    "    :type X: np.ndarray(shape=(N, d))\n",
    "    :param y: regression targets\n",
    "    :type y: np.ndarray(shape=(N, 1))\n",
    "    :param w: weights\n",
    "    :type w: np.array(shape=(d, 1))\n",
    "    :param learning_rate: learning rate\n",
    "    :type learning_rate: float\n",
    "    :param num_iters: number of iterations\n",
    "    :type num_iters: int\n",
    "    :return: weights, weights history, cost history\n",
    "    :rtype: np.array(shape=(d, 1)), list, list\n",
    "    \"\"\"\n",
    "    \n",
    "    weights_history = [w.flatten()]\n",
    "    cost_history = [compute_cost(X, y, w)]\n",
    "\n",
    "    # YOUR CODE HERE:\n",
    "    raise NotImplementedError\n",
    "    # END YOUR CODE\n",
    "\n",
    "    return w, weights_history, cost_history "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Teste exercício 4)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    learning_rate = 0.8\n",
    "    iterations = 20000\n",
    "    init = time.time()\n",
    "    w, weights_history, cost_history = batch_gradient_descent(train_X_1,\n",
    "                                                              train_y_norm,\n",
    "                                                              initial_w,\n",
    "                                                              learning_rate,\n",
    "                                                              iterations)\n",
    "    assert cost_history[-1] < cost_history[0]\n",
    "    assert type(w) == np.ndarray\n",
    "    assert len(weights_history) == len(cost_history)\n",
    "    init = time.time() - init\n",
    "    print(\"Tempo de treinamento = {:.8f}(s)\".format(init))\n",
    "    print(\"Tem que ser em menos de 1 segundo \")\n",
    "    \n",
    "except (NotImplementedError, NameError):\n",
    "    print(\"Falta fazer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Agora podemos treinar o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    learning_rate = 0.03\n",
    "    iterations = 400\n",
    "    w, weights_history, cost_history = batch_gradient_descent(train_X_1,\n",
    "                                                              train_y_norm,\n",
    "                                                              initial_w,\n",
    "                                                              learning_rate,\n",
    "                                                              iterations)\n",
    "    title = \"Optimization landscape\\nlearning rate = {} | iterations = {}\".format(learning_rate,\n",
    "                                                                                  iterations)\n",
    "    plot_cost_function_curve(train_X_1,\n",
    "                             train_y_norm,\n",
    "                             compute_cost,\n",
    "                             title=title,\n",
    "                             weights_list=weights_history,\n",
    "                             cost_list=cost_history)\n",
    "    simple_step_plot([cost_history],\n",
    "                 \"loss\",\n",
    "                 'Training loss\\nlearning rate = {} | iterations = {}'.format(learning_rate,\n",
    "                                                                              iterations))\n",
    "except (NotImplementedError, NameError):\n",
    "    print(\"Falta fazer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hiper parâmetros (*hyperparameters*)\n",
    "\n",
    "\n",
    "Hiper parâmetros são parâmetros que controlam o comportamento do algoritmo. Eles não são modificados pelo algoritmo de aprendizado. Escolhemos os hiper parâmetros de acordo com a performance deles no dataset de treinamento. Para evitar que o modelo decore o dataset de treinamento, pegamos uma parte desse dataset só para achar os melhores hiper parâmetros. Essa parte é chamada de **dataset de validação** (*validation set*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    valid_X_norm = standardize(valid_X)\n",
    "    valid_y_norm = standardize(valid_y)\n",
    "    valid_X_1 = add_feature_ones(valid_X_norm)\n",
    "\n",
    "    hyper_params = [(0.001, 200),\n",
    "                    (0.1, 10),\n",
    "                    (0.9, 8),\n",
    "                    (0.02, 600)]\n",
    "\n",
    "    all_costs = []\n",
    "    all_w = []\n",
    "\n",
    "    for param in hyper_params:\n",
    "        learning_rate = param[0]\n",
    "        iterations = param[1]\n",
    "        w, weights_history, cost_history = batch_gradient_descent(train_X_1,\n",
    "                                                                  train_y_norm,\n",
    "                                                                  initial_w,\n",
    "                                                                  learning_rate,\n",
    "                                                                  iterations)\n",
    "        all_costs.append(compute_cost(valid_X_1, valid_y_norm, w))\n",
    "        all_w.append(w)\n",
    "        title = \"Optimization landscape\\n\"\n",
    "        title += \"learning rate = {} | iterations = {}\".format(learning_rate,\n",
    "                                                               iterations)\n",
    "\n",
    "        plot_cost_function_curve(train_X_1,\n",
    "                                 train_y_norm,\n",
    "                                 compute_cost,\n",
    "                                 title=title,\n",
    "                                 weights_list=weights_history,\n",
    "                                 cost_list=cost_history)\n",
    "\n",
    "\n",
    "    best_result_i = np.argmin(all_costs)\n",
    "    best_w = all_w[best_result_i]\n",
    "    lowest_cost = all_costs[best_result_i]\n",
    "    best_params = hyper_params[best_result_i]\n",
    "\n",
    "    result_str = \"Best hyperparameters\\n\"\n",
    "    result_str += \"learning rate = {}\".format(best_params[0])\n",
    "    result_str += \" | iterations = {}\\n\".format(best_params[1])\n",
    "    result_str += \"w = {}\\n\".format(best_w.flatten())\n",
    "    result_str += \"lowest validation set cost = {}\\n\".format(lowest_cost)\n",
    "\n",
    "    print(result_str)\n",
    "\n",
    "except (NotImplementedError, NameError):\n",
    "    print(\"Falta fazer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Com o modelo treinado e escolhidos os melhores hiper parâmetros, podemos avaliá-lo sobre o dataset de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    test_X_norm = standardize(test_X)\n",
    "    test_y_norm = standardize(test_y)\n",
    "    test_X_1 = add_feature_ones(test_X_norm)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    prediction = linear_regression_prediction(test_X_1, best_w)\n",
    "    prediction = (prediction * np.std(train_y)) + np.mean(train_y)\n",
    "    r_2 = r_squared(test_y, prediction)\n",
    "\n",
    "    plot_points_regression(test_X,\n",
    "                           test_y,\n",
    "                           title='Test data',\n",
    "                           xlabel=\"m\\u00b2\",\n",
    "                           ylabel='$',\n",
    "                           prediction=prediction,\n",
    "                           r_squared=r_2,\n",
    "                           legend=True)\n",
    "\n",
    "except (NotImplementedError, NameError):\n",
    "    print(\"Falta fazer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradiente descendente estocástico\n",
    "\n",
    "Nos casos em que $N$ é um número grande, computar $\\nabla_{\\mathbf{w}}J(\\mathbf{w})$ a cada iteração se torna algo muito custoso. Uma estratégia para lidar com isso é **aproximar** $\\nabla_{\\mathbf{w}}J(\\mathbf{w})$ usando o gradiente:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\nabla_{\\mathbf{w}}J(\\mathbf{w})} = \\nabla_{\\mathbf{w}}\\frac{1}{m}\\sum_{i=1}^{m} L(h(\\mathbf{x}_{i}; \\mathbf{w}), \\; y_{i})\n",
    "\\end{equation}\n",
    "\n",
    "em que $(\\mathbf{x}_{1}, y_{1}), \\dots ,(\\mathbf{x}_{m}, y_{m})$ é uma amostragem aleatória dos dados de treinamento. A  estocasticidade surge da escolha desses $m$ dados (para que $\\hat{\\nabla_{\\mathbf{w}}J(\\mathbf{w})}$ seja um estimador não enviesado de $\\nabla_{\\mathbf{w}}J(\\mathbf{w})$ nós amostramos os $m$ dados a cada iteração). Normalmente usamos o nome **gradiente descendente estocástico** (*stochastic gradient descent* ou *online gradient descent*) quando $m=1$, e usamos o nome **minibatch stochastic gradient descent** quando $1 < m <N$ (nesse caso estamos usando apenas um pequeno lote dos dados, um *minibatch*). Usamos *batch* para referir a um *minibatch*, não confunda isso com *batch gradient descent*.\n",
    "\n",
    "\n",
    "**Stochastic gradient descent (SGD)**\n",
    "\n",
    "- $\\mathbf{w}(0) = \\mathbf{w}$\n",
    "- for $t = 0, 1, 2, \\dots$ do\n",
    "    * Sample a minibatch of $m$ examples from the training data.\n",
    "    * Compute the gradient estimate $\\hat{\\nabla_{\\mathbf{w}(t)}J(\\mathbf{w}(t))}$\n",
    "    * Apply update : $\\mathbf{w}(t+1) = \\mathbf{w}(t) - \\eta \\hat{\\nabla_{\\mathbf{w}(t)}J(\\mathbf{w}(t))}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  **Exercício 5)** \n",
    "Implemente o algoritmo stochastic gradient descent para a regressão linear com a taxa de apreendizado fixa. A saída da função é a mesma da função do exercício 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(X, y, w, learning_rate, num_iters, batch_size):\n",
    "    \"\"\"\n",
    "     Performs stochastic gradient descent optimization\n",
    "\n",
    "    :param X: design matrix\n",
    "    :type X: np.ndarray(shape=(N, d))\n",
    "    :param y: regression targets\n",
    "    :type y: np.ndarray(shape=(N, 1))\n",
    "    :param w: weights\n",
    "    :type w: np.array(shape=(d, 1))\n",
    "    :param learning_rate: learning rate\n",
    "    :type learning_rate: float\n",
    "    :param num_iters: number of iterations\n",
    "    :type num_iters: int\n",
    "    :param batch_size: size of the minibatch\n",
    "    :type batch_size: int\n",
    "    :return: weights, weights history, cost history\n",
    "    :rtype: np.array(shape=(d, 1)), list, list\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE:\n",
    "    raise NotImplementedError\n",
    "    # END YOUR CODE\n",
    "\n",
    "    return w, weights_history, cost_history "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Teste exercício 5)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    init = time.time()\n",
    "    learning_rate = 0.8\n",
    "    iterations = 2000\n",
    "    batch_size = 36\n",
    "    w, weights_history, cost_history = stochastic_gradient_descent(train_X_1,\n",
    "                                                                   train_y_norm,\n",
    "                                                                   initial_w,\n",
    "                                                                   learning_rate,\n",
    "                                                                   iterations,\n",
    "                                                                   batch_size)\n",
    "    assert cost_history[-1] < cost_history[0]\n",
    "    assert type(w) == np.ndarray\n",
    "    assert len(weights_history) == len(cost_history)\n",
    "    init = time.time() - init\n",
    "    print(\"Tempo de treinamento = {:.8f}(s)\".format(init))\n",
    "    print(\"Tem que ser em menos de 1.2 segundos\")\n",
    "    \n",
    "except (NotImplementedError, NameError):\n",
    "    print(\"Falta fazer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Podemos experimentar com diferentes tamanhos de batch para ver que quanto maior o tamanho do batch (mais próximo de $N$) menor a variância."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:    \n",
    "    hyper_params = [(0.001, 1000, 1),\n",
    "                    (0.001, 1000, 10),\n",
    "                    (0.001, 1000, 36)]\n",
    "    all_costs = []\n",
    "\n",
    "    for param in hyper_params:\n",
    "        learning_rate = param[0]\n",
    "        iterations = param[1]\n",
    "        batch_size = param[2]\n",
    "        _, weights_history, cost_history = stochastic_gradient_descent(train_X_1,\n",
    "                                                                       train_y_norm,\n",
    "                                                                       initial_w,\n",
    "                                                                       learning_rate,\n",
    "                                                                       iterations,\n",
    "                                                                       batch_size)\n",
    "        all_costs.append(cost_history)\n",
    "        title = \"Optimization landscape\\n\"\n",
    "        title += \"learning rate = {}\".format(learning_rate)\n",
    "        title += \" | iterations = {}\".format(iterations)\n",
    "        title += \" | batch size = {}\".format(batch_size)\n",
    "        plot_cost_function_curve(train_X_1,\n",
    "                                 train_y_norm,\n",
    "                                 compute_cost,\n",
    "                                 title=title,\n",
    "                                 weights_list=weights_history,\n",
    "                                 cost_list=cost_history)\n",
    "\n",
    "\n",
    "    _, _, cost_history_full = batch_gradient_descent(train_X_1,\n",
    "                                                     train_y_norm,\n",
    "                                                     initial_w,\n",
    "                                                     learning_rate=0.001,\n",
    "                                                     num_iters=1000)\n",
    "\n",
    "    all_costs.append(cost_history_full)\n",
    "    labels_size = [\"batch size = \" + str(param[2]) for param in hyper_params]\n",
    "    labels_size += [\"batch size = \" + str(train_X_1.shape[0])]\n",
    "\n",
    "    simple_step_plot(all_costs,\n",
    "                     \"loss\",\n",
    "                     'Training loss',\n",
    "                      figsize=(8, 8),\n",
    "                      labels=labels_size)\n",
    "\n",
    "\n",
    "except (NotImplementedError, NameError):\n",
    "    print(\"Falta fazer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Por que normalizar?\n",
    "\n",
    "O primeiro motivo para se normalizar os dados é para evitar *overflow*. Também é verdade que quando não normalizamos os dados as *features* podem apresentar diferentes escalas -- note que esse é o caso nesse dataset em que uma *feature* só tem $1$s e a outra ($m^{2}$) apresenta bastante variação. Isso influencia no gradiente de modo que a cada atualização os valores dos pesos vão mudar de modo diferente mesmo usando o mesmo *learning rate*.\n",
    "\n",
    "Isso pode ser visto quando acompanhamos a mudança nos pesos ao longo do treinamento no dataset original e no normalizado. Note como o parâmetro $\\mathbf{w}[1]$ (que pondera a feature ($m^{2}$)) muda bem mais que o parâmetro $\\mathbf{w}[0]$ quando usamos o dataset não normalizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:    \n",
    "    _, weights_history_norm, cost_history_norm = batch_gradient_descent(train_X_1,\n",
    "                                                                        train_y_norm,\n",
    "                                                                        initial_w,\n",
    "                                                                        learning_rate,\n",
    "                                                                        10)\n",
    "\n",
    "    train_X_1_non_norm = add_feature_ones(train_X)\n",
    "\n",
    "    w, weights_history, cost_history = batch_gradient_descent(train_X_1_non_norm,\n",
    "                                                              train_y,\n",
    "                                                              initial_w,\n",
    "                                                              0.000002,\n",
    "                                                              10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    w0_hist_norm = [w[0] for w in weights_history_norm]\n",
    "    w1_hist_norm = [w[1] for w in weights_history_norm]\n",
    "\n",
    "\n",
    "    w0mean, w0sdt, w0max, w0min = np.mean(w0_hist_norm), np.std(w0_hist_norm), np.max(w0_hist_norm), np.min(w0_hist_norm)\n",
    "    w1mean, w1sdt, w1max, w1min = np.mean(w0_hist_norm), np.std(w1_hist_norm), np.max(w1_hist_norm), np.min(w1_hist_norm)\n",
    "\n",
    "    print(\"\\nVariação dos pesos com o dataset normalizado\\n\")\n",
    "    print(\"w[0]:\\nmean {}, std {:.2f}, max {}, min {}\".format(w0mean, w0sdt, w0max, w0min))\n",
    "    print(\"w[1]:\\nmean {}, *std {:.2f}*, max {}, min {}\".format(w1mean, w1sdt, w1max, w1min))              \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    w0_hist = [w[0] for w in weights_history]\n",
    "    w1_hist = [w[1] for w in weights_history]\n",
    "\n",
    "\n",
    "    w0mean, w0sdt, w0max, w0min = np.mean(w0_hist), np.std(w0_hist), np.max(w0_hist), np.min(w0_hist)\n",
    "    w1mean, w1sdt, w1max, w1min = np.mean(w1_hist), np.std(w1_hist), np.max(w1_hist), np.min(w1_hist)\n",
    "\n",
    "    print(\"\\nVariação dos pesos com o dataset não normalizado\\n\")\n",
    "    print(\"w[0]:\\nmean {}, std {:.2f}, max {}, min {}\".format(w0mean, w0sdt, w0max, w0min))\n",
    "    print(\"w[1]:\\nmean {}, *std {:.2f}*, max {}, min {}\".format(w1mean, w1sdt, w1max, w1min))\n",
    "\n",
    "except (NotImplementedError, NameError):\n",
    "    print(\"Falta fazer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Um dos resultados dessa atualização em scala diferente para cada feature é a não convergência do algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:   \n",
    "    simple_step_plot([cost_history_norm],\n",
    "                     \"loss\",\n",
    "                     'Training loss (normalized)')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    simple_step_plot([cost_history],\n",
    "                     \"loss\",\n",
    "                     'Training loss (non normalized)')\n",
    "\n",
    "\n",
    "\n",
    "    plot_cost_function_curve(train_X_1_non_norm,\n",
    "                             train_y,\n",
    "                             compute_cost,\n",
    "                             title=\"Optimization landscape\\n(non normalized data)\",\n",
    "                             weights_list=weights_history,\n",
    "                             cost_list=cost_history,\n",
    "                             range_points=(100, 100))\n",
    "except (NotImplementedError, NameError):\n",
    "    print(\"Falta fazer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mais otimização!\n",
    "\n",
    "Há muitos outros algoritmos de otimização construídos em cima da ideia de gradiente descendente. Um bom resumo de alguns desses algoritimos pode ser encontrado [aqui](http://ruder.io/optimizing-gradient-descent/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
