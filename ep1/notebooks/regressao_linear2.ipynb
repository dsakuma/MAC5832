{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão linear 2:  gradiente descendente\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aprendizado e otimização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dada uma função *target* desconhecida $f:\\mathbb{R}^{d} \\rightarrow \\mathbb{R}$ e uma hipótese $h_{\\mathbf{w}}:\\mathbb{R}^{d} \\rightarrow \\mathbb{R}$ o erro de $h_{\\mathbf{w}}$ é definido por:\n",
    "\n",
    "\\begin{equation}\n",
    "E_{out}(h_{\\mathbf{w}}) = \\mathbb{E}_{\\mathbf{x}\\sim p_{data}}L(h(\\mathbf{x}; \\mathbf{w}), \\; f(\\mathbf{x}))\n",
    "\\end{equation}\n",
    "\n",
    "em que $p_{data}$ é a distribuição geradora dos dados, $L$  é alguma função de perda (por exemplo, o quadrado da diferença) e $h(\\mathbf{x}; \\mathbf{w}) = h_{\\mathbf{w}}(\\mathbf{x})$. Se tivéssemos acesso a $p_{data}$ poderíamos calcular a função acima para qualquer $h_{\\mathbf{w}}$ e escolher aquele com erro mínimo.\n",
    "\n",
    "Como não temos acesso a $p_{data}$, define-se\n",
    "\n",
    "\\begin{equation}\n",
    "E_{in}(h_{\\mathbf{w}}) = J(\\mathbf{w}) = \\frac{1}{N}\\sum_{i=1}^{N} L(h(\\mathbf{x}_{i}; \\mathbf{w}), \\; y_{i})\n",
    "\\end{equation}\n",
    "\n",
    "em que $N$ é o tamanho do dataset de treinamento e $y_{i} = f(\\mathbf{x}_{i})$.\n",
    "\n",
    "Como visto em aula, uma relação entre $E_{in}$ e $E_{out}$ pode ser estabelecida pela **Inequação de Hoeffding**.\n",
    "\n",
    "Aqui estamos interessados em encontrar $h_{\\mathbf{w}}$ com erro $J(\\mathbf{w})$ mínimo. Isto corresponde a determinar o ponto mínimo da função $J(\\mathbf{w})$. Por isso, vamos nos concentrar apenas em uma técnica de otimização para minimizar $E_{in}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradiente ascendente e gradiente descendente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dados $\\mathbf{w}, \\mathbf{u} \\in \\mathbb{R}^{d}$ e $J:\\mathbb{R}^{d} \\rightarrow \\mathbb{R}$ tal que $||\\mathbf{u}||_{2} = 1$ a taxa de variação de $J$ no ponto $\\mathbf{w}$ em direção a $\\mathbf{u}$ é chamada de **derivada direcional**, $D_{\\mathbf{u}}J(\\mathbf{w})$. Definindo $g(h) = J(\\mathbf{w} + h\\mathbf{u})$, podemos usar a regra da cadeia e a definição de produto escalar para mostrar que  \n",
    "\n",
    "\\begin{equation}\n",
    "D_{\\mathbf{u}}J(\\mathbf{w}) \\;\\;=\\;\\; \\left. \\frac{dg}{dh} \\right|_{h=0} \\;\\;=\\;\\; \\mathbf{u}^{T}\\nabla_{\\mathbf{w}}J(\\mathbf{w})\n",
    "    \\;\\;=\\;\\; ||\\mathbf{u}||_{2}||\\nabla_{\\mathbf{w}}J(\\mathbf{w})||_{2}cos\\theta \\;\\;=\\;\\; ||\\nabla_{\\mathbf{w}}J(\\mathbf{w})||_{2}cos\\theta\n",
    "\\end{equation}\n",
    "\n",
    "em que $\\theta$ é o ângulo entre $\\mathbf{u}$ e $\\nabla_{\\mathbf{w}}J(\\mathbf{w})$. Assim, pensando em $D_{\\mathbf{u}}J(\\mathbf{w})$ em termos do vetor $\\mathbf{u}$ temos que: $D_{\\mathbf{u}}J(\\mathbf{w})$ tem o valor máximo quando $\\mathbf{u}$ tem a mesma direção que $\\nabla_{\\mathbf{w}}J(\\mathbf{w})$ ($\\theta=0$); similarmente $D_{\\mathbf{u}}J(\\mathbf{w})$ tem o valor mínimo quando $\\mathbf{u}$ tem a direção oposta a $\\nabla_{\\mathbf{w}}J(\\mathbf{w})$ ($\\theta=\\pi$).\n",
    "\n",
    "Desse modo podemos maximizar $J$ alterando $\\mathbf{w}$ na direção de $\\nabla_{\\mathbf{w}}J(\\mathbf{w})$ e podemos minimizar $J$ alterando $\\mathbf{w}$ na direção de $-\\nabla_{\\mathbf{w}}J(\\mathbf{w})$. Isso permite dois métodos bem simples de otimização:\n",
    "\n",
    "**Gradiente Ascendente**\n",
    "\n",
    "- $\\mathbf{w}(0) = \\mathbf{w}$\n",
    "- for $t = 0, 1, 2, \\dots$ do\n",
    "    * $\\mathbf{w}(t+1) = \\mathbf{w}(t) + \\eta \\nabla_{\\mathbf{w}(t)}J(\\mathbf{w}(t))$ \n",
    "\n",
    "    \n",
    "\n",
    "**Gradiente Descendente**\n",
    "\n",
    "- $\\mathbf{w}(0) = \\mathbf{w}$\n",
    "- for $t = 0, 1, 2, \\dots$ do\n",
    "    * $\\mathbf{w}(t+1) = \\mathbf{w}(t) - \\eta \\nabla_{\\mathbf{w}(t)}J(\\mathbf{w}(t))$ \n",
    "\n",
    "Em ambos os casos o parâmetro $\\eta \\in \\mathbb{R}_{\\geq}$ é chamado de **taxa de aprendizado** (*learning rate*); ele pondera o tamanho de cada atualização."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####   A seguir são apresentados uma sequência de exercícios que ilustram diferentes aspectos práticos na implementação do algoritmo *gradient descent*  \n",
    "Iremos considerar o problema de regressão linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all imports\n",
    "import numpy as np\n",
    "import time\n",
    "from util import r_squared, randomize_in_place, get_housing_prices_data\n",
    "from plots import simple_step_plot, plot_points_regression, plot_cost_function_curve\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Vamos usar o mesmo dataset de antes, mas agora vamos dividir os dados em treinamento, validação e teste.\n",
    "Essa divisão é comumente realizada na prática: os dados de treinamento são usados na otimização da função custo $J$, os dados de validação são usados para aferir a qualidade da otimização, e os dados de teste são usados para aferir a qualidade da predição."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dividindo os dados em treinamento, validação e teste\n",
      "\n",
      "train_X shape = (250, 1)\n",
      "\n",
      "train_y shape = (250, 1)\n",
      "\n",
      "valid_X shape = (50, 1)\n",
      "\n",
      "valid_y shape = (50, 1)\n",
      "\n",
      "test_X shape = (50, 1)\n",
      "\n",
      "test_y shape = (50, 1)\n"
     ]
    }
   ],
   "source": [
    "X, y = get_housing_prices_data(N=350, verbose=False)\n",
    "randomize_in_place(X, y)\n",
    "\n",
    "train_X = X[0:250]\n",
    "train_y = y[0:250]\n",
    "valid_X = X[250:300]\n",
    "valid_y = y[250:300]\n",
    "test_X = X[300:]\n",
    "test_y = y[300:]\n",
    "\n",
    "print(\"\\nDividindo os dados em treinamento, validação e teste\")\n",
    "print(\"\\ntrain_X shape = {}\".format(train_X.shape))\n",
    "print(\"\\ntrain_y shape = {}\".format(train_y.shape))\n",
    "print(\"\\nvalid_X shape = {}\".format(valid_X.shape))\n",
    "print(\"\\nvalid_y shape = {}\".format(valid_y.shape))\n",
    "print(\"\\ntest_X shape = {}\".format(test_X.shape))\n",
    "print(\"\\ntest_y shape = {}\".format(test_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1059ec5be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_points_regression(train_X,\n",
    "                       train_y,\n",
    "                       title='Training data',\n",
    "                       xlabel=\"m\\u00b2\",\n",
    "                       ylabel='$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para minimizar a função de custo vamos ter que colocar os dados em uma outra escala\n",
    "\n",
    "Um modo de fazer isso é o chamado [standard/z score](https://en.wikipedia.org/wiki/Standard_score).\n",
    "Aplicamos a seguinte transformação: \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{X}^{\\top}_{i} \\leftarrow \\frac{\\mathbf{X}^{\\top}_{i} - \\mu_{i}}{\\sigma_{i}}\n",
    "\\end{equation}\n",
    "\n",
    "em que $\\mathbf{X}^{T}_{i} \\in \\mathbb{R}^{N}$ ($i = 1, \\dots, d$) é um vetor de features da design matrix $\\mathbf{X}$, $\\mu_{i}$ é a média de tal vetor, e $\\sigma_{i}$ seu desvio padrão.\n",
    "\n",
    "A importância de se fazer essa transformação é discutida mais ao final deste notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  **Exercício 1)** \n",
    "Use a biblioteca numpy para implementar a função que altera os dados conforme a equação acima (essa função deve funcionar para uma design matrix $\\mathbf{X}$ com um número arbitrário de features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(X):\n",
    "    \"\"\"\n",
    "    Returns standardized version of the ndarray 'X'.\n",
    "\n",
    "    :param X: input array\n",
    "    :type X: np.ndarray(shape=(N, d))\n",
    "    :return: standardized array\n",
    "    :rtype: np.ndarray(shape=(N, d))\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE:\n",
    "    mean = X.mean(axis=0)\n",
    "    std_dev = X.std(axis=0)\n",
    "    X_out = (X-mean)/std_dev\n",
    "    # END YOUR CODE\n",
    "\n",
    "    return X_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Teste exercício 1)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    toy_X = np.array([[1100.3, 2.4, 34.34],\n",
    "                      [2300.3, 1.4, 442.23]])\n",
    "    toy_y = np.array([[1000.2], [2000.5]])\n",
    "    toy_X_norm = standardize(toy_X)\n",
    "    toy_y_norm = standardize(toy_y)\n",
    "    xmean, xstd = np.mean(toy_X_norm), np.std(toy_X_norm)\n",
    "    ymean, ystd = np.mean(toy_y_norm), np.std(toy_y_norm)\n",
    "    assert -1 <= xmean < 0\n",
    "    assert 0 <= ymean < 1\n",
    "    assert 0.9 <= xstd <= 1\n",
    "    assert 0.9 <= ystd <= 1\n",
    "\n",
    "except NotImplementedError:\n",
    "    print(\"Falta fazer!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados originais\n",
      "\n",
      "X:\n",
      "mean 636.5652465820312, std 323.76, max 1200.0, min 93.1805191040039\n",
      "\n",
      "y:\n",
      "mean 43632.046875, std 16909.14, max 77118.890625, min 10827.3046875\n",
      "\n",
      "Dados normalizados\n",
      "\n",
      "X:\n",
      "mean 7.915496524901755e-08, std 1.00, max 1.740271806716919, min -1.6783435344696045\n",
      "\n",
      "y:\n",
      "mean -8.583068478174027e-09, std 1.00, max 1.9803993701934814, min -1.9400601387023926\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1059ddc908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    train_X_norm = standardize(train_X)\n",
    "    train_y_norm = standardize(train_y)\n",
    "\n",
    "\n",
    "    xmean, xstd = np.mean(train_X), np.std(train_X)\n",
    "    xmax, xmin = np.max(train_X), np.min(train_X)\n",
    "    ymean, ystd = np.mean(train_y), np.std(train_y)\n",
    "    ymax, ymin = np.max(train_y), np.min(train_y)\n",
    "\n",
    "    print(\"Dados originais\\n\")\n",
    "    print(\"X:\\nmean {}, std {:.2f}, max {}, min {}\".format(xmean,\n",
    "                                                           xstd,\n",
    "                                                           xmax,\n",
    "                                                           xmin))\n",
    "    print(\"\\ny:\\nmean {}, std {:.2f}, max {}, min {}\\n\".format(ymean,\n",
    "                                                             ystd,\n",
    "                                                             ymax,\n",
    "                                                             ymin))\n",
    "\n",
    "\n",
    "    xmean, xstd = np.mean(train_X_norm), np.std(train_X_norm)\n",
    "    xmax, xmin = np.max(train_X_norm), np.min(train_X_norm)\n",
    "    ymean, ystd = np.mean(train_y_norm), np.std(train_y_norm)\n",
    "    ymax, ymin = np.max(train_y_norm), np.min(train_y_norm)\n",
    "\n",
    "    print(\"Dados normalizados\\n\")\n",
    "    print(\"X:\\nmean {}, std {:.2f}, max {}, min {}\".format(xmean,\n",
    "                                                           xstd,\n",
    "                                                           xmax,\n",
    "                                                           xmin))\n",
    "    print(\"\\ny:\\nmean {}, std {:.2f}, max {}, min {}\\n\".format(ymean,\n",
    "                                                             ystd,\n",
    "                                                             ymax,\n",
    "                                                             ymin))\n",
    "    plot_points_regression(train_X_norm,\n",
    "                           train_y_norm,\n",
    "                           title='Training data (normalized)',\n",
    "                           xlabel=\"m\\u00b2\",\n",
    "                           ylabel='$')\n",
    "\n",
    "except NotImplementedError:\n",
    "    print(\"Falta fazer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Adicionando uma componente com apenas 1s como uma nova feature\n",
    "Conforme já vimos, adicionar uma componente (coordenada artificial) constante 1 é conveniente. Isto é, em vez de $\\mathbf{x} \\in \\mathbb{R}^d$ é conveniente considerarmos $(1,\\mathbf{x}) \\in \\mathbb{R}^{d+1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature_ones(X):\n",
    "    \"\"\"\n",
    "    Returns the ndarray 'X' with the extra\n",
    "    feature column containing only 1s.\n",
    "\n",
    "    :param X: input array\n",
    "    :type X: np.ndarray(shape=(N, d))\n",
    "    :return: output array\n",
    "    :rtype: np.ndarray(shape=(N, d+1))\n",
    "    \"\"\"\n",
    "    return np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
    "\n",
    "try:\n",
    "    train_X_1 = add_feature_ones(train_X_norm)\n",
    "    print(\"\\ntrain_X shape = {}\".format(train_X_1.shape))\n",
    "\n",
    "except NameError:\n",
    "    print(\"Falta fazer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Criando a predição da regressão linear e plotando uma predição arbitrária"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_prediction(X, w):\n",
    "    \"\"\"\n",
    "    Calculates the linear regression prediction.\n",
    "\n",
    "    :param X: design matrix\n",
    "    :type X: np.ndarray(shape=(N, d))\n",
    "    :param w: weights\n",
    "    :type w: np.array(shape=(d, 1))\n",
    "    :return: prediction\n",
    "    :rtype: np.array(shape=(N, 1))\n",
    "    \"\"\"\n",
    "\n",
    "    return X.dot(w)\n",
    "\n",
    "try:\n",
    "    w = np.array([[1.2], [2.3]])\n",
    "    prediction = linear_regression_prediction(train_X_1, w)\n",
    "\n",
    "    plot_points_regression(train_X_norm,\n",
    "                           train_y_norm,\n",
    "                           title='Training data (normalized)',\n",
    "                           xlabel=\"m\\u00b2\",\n",
    "                           ylabel='$',\n",
    "                           prediction=prediction,\n",
    "                           legend=True)\n",
    "except NameError:\n",
    "    print(\"Falta fazer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computando a função de custo\n",
    "\n",
    "Usando o erro quadrárico médio, a função de custo $J(\\mathbf{w})$ para a tarefa de regressão linear pode ser escrita de dois modos. A forma iterativa:\n",
    "\n",
    "\\begin{equation}\n",
    "J(\\mathbf{w}) = \\frac{1}{N}\\sum_{i=1}^{N}(\\hat{y}_{i} - y_{i})^{2}\n",
    "\\end{equation}\n",
    "\n",
    "e a forma vetorial:\n",
    "\n",
    "\\begin{equation}\n",
    "J(\\mathbf{w}) = \\frac{1}{N}(\\mathbf{X}\\mathbf{w} - \\mathbf{y})^{T}(\\mathbf{X}\\mathbf{w} - \\mathbf{y})\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  **Exercício 2)**  \n",
    "Use a biblioteca numpy para implementar a função de custo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(X, y, w):\n",
    "    \"\"\"\n",
    "    Calculates  mean square error cost.\n",
    "\n",
    "    :param X: design matrix\n",
    "    :type X: np.ndarray(shape=(N, d))\n",
    "    :param y: regression targets\n",
    "    :type y: np.ndarray(shape=(N, 1))\n",
    "    :param w: weights\n",
    "    :type w: np.array(shape=(d, 1))\n",
    "    :return: cost\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE:\n",
    "    raise NotImplementedError\n",
    "    # END YOUR CODE\n",
    "\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Teste exercício 2)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    toy_w = np.array([[1], [1], [2]])\n",
    "    toy_X = np.array([[2, 3, 1],\n",
    "                      [5, 1, 2]])\n",
    "    toy_y = np.array([[1], [1]])\n",
    "    assert compute_cost(toy_X, toy_y, toy_w) == 58.5\n",
    "except NotImplementedError:\n",
    "    print(\"Falta fazer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos olhar a superficie de custo e ver onde se situa um valor $J(\\mathbf{w})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:    \n",
    "    initial_w = np.array([[15], [-35.3]])\n",
    "    initial_J = compute_cost(train_X_1, train_y_norm, initial_w)\n",
    "\n",
    "    plot_cost_function_curve(train_X_1,\n",
    "                             train_y_norm,\n",
    "                             compute_cost,\n",
    "                             title=\"Optimization landscape\",\n",
    "                             weights_list=[initial_w.flatten()],\n",
    "                             cost_list=[initial_J])\n",
    "except (NotImplementedError, NameError):\n",
    "    print(\"Falta fazer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculando os gradientes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É fácil calcular a derivada parcial de $J(\\mathbf{w})$ com relação a cada entrada $j$ de $\\mathbf{w}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial J(\\mathbf{w})}{\\partial \\mathbf{w}_{j}} = \\frac{2}{N}\\sum_{i=1}^{N} (\\hat{y}_i - y_i) \\mathbf{x}_{ij}\n",
    "\\end{equation}\n",
    "\n",
    "Lembre que o gradiente de $J(\\mathbf{w})$ com relação a $\\mathbf{w}$ é:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_{\\mathbf{w}}J(\\mathbf{w}) = \\begin{bmatrix}\\frac{\\partial J(\\mathbf{w})}{\\partial \\mathbf{w}_{1}} \\dots \\frac{\\partial J(\\mathbf{w})}{\\partial \\mathbf{w}_{m}} \\end{bmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  **Exercício 3)**  \n",
    "Use a biblioteca numpy para calcular $\\nabla_{\\mathbf{w}}J(\\mathbf{w})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_wgrad(X, y, w):\n",
    "    \"\"\"\n",
    "    Calculates gradient of J(w) with respect to w.\n",
    "\n",
    "    :param X: design matrix\n",
    "    :type X: np.ndarray(shape=(N, d))\n",
    "    :param y: regression targets\n",
    "    :type y: np.ndarray(shape=(N, 1))\n",
    "    :param w: weights\n",
    "    :type w: np.array(shape=(d, 1))\n",
    "    :return: gradient\n",
    "    :rtype: np.array(shape=(d, 1))\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE:\n",
    "    raise NotImplementedError\n",
    "    # END YOUR CODE\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Teste exercício 3)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_check(X, y, w, h=1e-4):\n",
    "    \"\"\"\n",
    "    Check gradients for linear regression.\n",
    "\n",
    "    :param X: design matrix\n",
    "    :type X: np.ndarray(shape=(N, d))\n",
    "    :param y: regression targets\n",
    "    :type y: np.ndarray(shape=(N, 1))\n",
    "    :param w: weights\n",
    "    :type w: np.array(shape=(d, 1))\n",
    "    :param h: small variation\n",
    "    :type h: float\n",
    "    :return: gradient test\n",
    "    :rtype: boolean\n",
    "    \"\"\"\n",
    "    Jw = compute_cost(X, y, w)\n",
    "    grad = compute_wgrad(X, y, w)\n",
    "    passing = True\n",
    "    d = w.shape[0]\n",
    "    for i in range(d):\n",
    "        w_plus_h = np.array(w, copy=True)\n",
    "        w_plus_h[i] = w_plus_h[i] + h\n",
    "        Jw_plus_h = compute_cost(X, y, w_plus_h)\n",
    "        w_minus_h = np.array(w, copy=True)\n",
    "        w_minus_h[i] = w_minus_h[i] - h\n",
    "        Jw_minus_h = compute_cost(X, y, w_minus_h)\n",
    "        numgrad_i = (Jw_plus_h - Jw_minus_h) / (2 * h)\n",
    "        reldiff = abs(numgrad_i - grad[i]) / max(1, abs(numgrad_i), abs(grad[i]))\n",
    "        if reldiff > 1e-5:\n",
    "            passing = False\n",
    "            msg = \"\"\"\n",
    "            Seu gradiente = {0}\n",
    "            Gradiente numérico = {1}\"\"\".format(grad[i], numgrad_i)\n",
    "            print(\"            \" + str(i) + \": \" + msg)\n",
    "            print(\"            Jw = {}\".format(Jw))\n",
    "            print(\"            Jw_plus_h = {}\".format(Jw_plus_h))\n",
    "            print(\"            Jw_minus_h = {}\\n\".format(Jw_minus_h))\n",
    "\n",
    "    if passing:\n",
    "        print(\"Gradiente passando!\")\n",
    "    \n",
    "    return passing \n",
    "\n",
    "try:\n",
    "    toy_w1 = np.array([[1.], [2.], [1.], [2.]])\n",
    "    toy_X1 = np.array([[2., 3., 1., 2.],\n",
    "                      [5., 1., 1., 2.]])\n",
    "    toy_y1 = np.array([[1.], [-1.]])\n",
    "    toy_w2 = np.array([[-100.22], [20002.1], [102.5]])\n",
    "    toy_X2 = np.array([[2111.3, -2223., 404.0],\n",
    "                      [5222., -22221., 3.3]])\n",
    "    toy_y2 = np.array([[122.], [221.]])\n",
    "    toy_w3 = np.array([[-10.22], [-3.1]])\n",
    "    toy_X3 = np.array([[1.3, -1.2],\n",
    "                      [2.2, -2.1],\n",
    "                      [-2.3, -5.5],\n",
    "                      [3.2, 8.1],\n",
    "                      [3.3, -1.1],\n",
    "                      [-3.4, -2.22],\n",
    "                      [2.23, -4.4],\n",
    "                      [5.2, -2.3]])\n",
    "    toy_y3 = np.array([[10.3],\n",
    "                       [23.3],\n",
    "                       [10.1],\n",
    "                       [-20.2],\n",
    "                       [-10.2],\n",
    "                       [20.2],\n",
    "                       [-14.4],\n",
    "                       [-30.3]])\n",
    "    \n",
    "    assert grad_check(toy_X1, toy_y1, toy_w1)\n",
    "    assert grad_check(toy_X2, toy_y2, toy_w2)\n",
    "    assert grad_check(toy_X3, toy_y3, toy_w3)\n",
    "\n",
    "except NotImplementedError:\n",
    "    print(\"Falta fazer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch gradient descent\n",
    "\n",
    "A versão mais simples do algoritmo *gradient descent* faz uso de todas as observações do dataset de treinamento (esse algoritmo também é conhecido como *batch gradient descent* ou *vanilla gradient descent*).\n",
    "\n",
    "**Batch gradient descent**\n",
    "\n",
    "- $\\mathbf{w}(0) = \\mathbf{w}$\n",
    "- for $t = 0, 1, 2, \\dots$ do\n",
    "    * Compute the gradient $\\nabla_{\\mathbf{w}(t)}J(\\mathbf{w}(t))$ \n",
    "    * Apply update : $\\mathbf{w}(t+1) = \\mathbf{w}(t) - \\eta \\nabla_{\\mathbf{w}(t)}J(\\mathbf{w}(t))$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  **Exercício 4)** \n",
    "Implemente o algoritmo batch gradient descent com a taxa de apreendizado fixa para a regressão linear. A função abaixo deve retornar três coisas: o vetor de pesos $\\mathbf{w}$, uma lista com cada peso obtido ao longo do treinamento, e uma lista com o custo de cada peso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_gradient_descent(X, y, w, learning_rate, num_iters):\n",
    "    \"\"\"\n",
    "     Performs batch gradient descent optimization.\n",
    "\n",
    "    :param X: design matrix\n",
    "    :type X: np.ndarray(shape=(N, d))\n",
    "    :param y: regression targets\n",
    "    :type y: np.ndarray(shape=(N, 1))\n",
    "    :param w: weights\n",
    "    :type w: np.array(shape=(d, 1))\n",
    "    :param learning_rate: learning rate\n",
    "    :type learning_rate: float\n",
    "    :param num_iters: number of iterations\n",
    "    :type num_iters: int\n",
    "    :return: weights, weights history, cost history\n",
    "    :rtype: np.array(shape=(d, 1)), list, list\n",
    "    \"\"\"\n",
    "    \n",
    "    weights_history = [w.flatten()]\n",
    "    cost_history = [compute_cost(X, y, w)]\n",
    "\n",
    "    # YOUR CODE HERE:\n",
    "    raise NotImplementedError\n",
    "    # END YOUR CODE\n",
    "\n",
    "    return w, weights_history, cost_history "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Teste exercício 4)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    learning_rate = 0.8\n",
    "    iterations = 20000\n",
    "    init = time.time()\n",
    "    w, weights_history, cost_history = batch_gradient_descent(train_X_1,\n",
    "                                                              train_y_norm,\n",
    "                                                              initial_w,\n",
    "                                                              learning_rate,\n",
    "                                                              iterations)\n",
    "    assert cost_history[-1] < cost_history[0]\n",
    "    assert type(w) == np.ndarray\n",
    "    assert len(weights_history) == len(cost_history)\n",
    "    init = time.time() - init\n",
    "    print(\"Tempo de treinamento = {:.8f}(s)\".format(init))\n",
    "    print(\"Tem que ser em menos de 1 segundo \")\n",
    "    \n",
    "except (NotImplementedError, NameError):\n",
    "    print(\"Falta fazer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Agora podemos treinar o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    learning_rate = 0.03\n",
    "    iterations = 400\n",
    "    w, weights_history, cost_history = batch_gradient_descent(train_X_1,\n",
    "                                                              train_y_norm,\n",
    "                                                              initial_w,\n",
    "                                                              learning_rate,\n",
    "                                                              iterations)\n",
    "    title = \"Optimization landscape\\nlearning rate = {} | iterations = {}\".format(learning_rate,\n",
    "                                                                                  iterations)\n",
    "    plot_cost_function_curve(train_X_1,\n",
    "                             train_y_norm,\n",
    "                             compute_cost,\n",
    "                             title=title,\n",
    "                             weights_list=weights_history,\n",
    "                             cost_list=cost_history)\n",
    "    simple_step_plot([cost_history],\n",
    "                 \"loss\",\n",
    "                 'Training loss\\nlearning rate = {} | iterations = {}'.format(learning_rate,\n",
    "                                                                              iterations))\n",
    "except (NotImplementedError, NameError):\n",
    "    print(\"Falta fazer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hiper parâmetros (*hyperparameters*)\n",
    "\n",
    "\n",
    "Hiper parâmetros são parâmetros que controlam o comportamento do algoritmo. Eles não são modificados pelo algoritmo de aprendizado. Escolhemos os hiper parâmetros de acordo com a performance deles no dataset de treinamento. Para evitar que o modelo decore o dataset de treinamento, pegamos uma parte desse dataset só para achar os melhores hiper parâmetros. Essa parte é chamada de **dataset de validação** (*validation set*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    valid_X_norm = standardize(valid_X)\n",
    "    valid_y_norm = standardize(valid_y)\n",
    "    valid_X_1 = add_feature_ones(valid_X_norm)\n",
    "\n",
    "    hyper_params = [(0.001, 200),\n",
    "                    (0.1, 10),\n",
    "                    (0.9, 8),\n",
    "                    (0.02, 600)]\n",
    "\n",
    "    all_costs = []\n",
    "    all_w = []\n",
    "\n",
    "    for param in hyper_params:\n",
    "        learning_rate = param[0]\n",
    "        iterations = param[1]\n",
    "        w, weights_history, cost_history = batch_gradient_descent(train_X_1,\n",
    "                                                                  train_y_norm,\n",
    "                                                                  initial_w,\n",
    "                                                                  learning_rate,\n",
    "                                                                  iterations)\n",
    "        all_costs.append(compute_cost(valid_X_1, valid_y_norm, w))\n",
    "        all_w.append(w)\n",
    "        title = \"Optimization landscape\\n\"\n",
    "        title += \"learning rate = {} | iterations = {}\".format(learning_rate,\n",
    "                                                               iterations)\n",
    "\n",
    "        plot_cost_function_curve(train_X_1,\n",
    "                                 train_y_norm,\n",
    "                                 compute_cost,\n",
    "                                 title=title,\n",
    "                                 weights_list=weights_history,\n",
    "                                 cost_list=cost_history)\n",
    "\n",
    "\n",
    "    best_result_i = np.argmin(all_costs)\n",
    "    best_w = all_w[best_result_i]\n",
    "    lowest_cost = all_costs[best_result_i]\n",
    "    best_params = hyper_params[best_result_i]\n",
    "\n",
    "    result_str = \"Best hyperparameters\\n\"\n",
    "    result_str += \"learning rate = {}\".format(best_params[0])\n",
    "    result_str += \" | iterations = {}\\n\".format(best_params[1])\n",
    "    result_str += \"w = {}\\n\".format(best_w.flatten())\n",
    "    result_str += \"lowest validation set cost = {}\\n\".format(lowest_cost)\n",
    "\n",
    "    print(result_str)\n",
    "\n",
    "except (NotImplementedError, NameError):\n",
    "    print(\"Falta fazer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Com o modelo treinado e escolhidos os melhores hiper parâmetros, podemos avaliá-lo sobre o dataset de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    test_X_norm = standardize(test_X)\n",
    "    test_y_norm = standardize(test_y)\n",
    "    test_X_1 = add_feature_ones(test_X_norm)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    prediction = linear_regression_prediction(test_X_1, best_w)\n",
    "    prediction = (prediction * np.std(train_y)) + np.mean(train_y)\n",
    "    r_2 = r_squared(test_y, prediction)\n",
    "\n",
    "    plot_points_regression(test_X,\n",
    "                           test_y,\n",
    "                           title='Test data',\n",
    "                           xlabel=\"m\\u00b2\",\n",
    "                           ylabel='$',\n",
    "                           prediction=prediction,\n",
    "                           r_squared=r_2,\n",
    "                           legend=True)\n",
    "\n",
    "except (NotImplementedError, NameError):\n",
    "    print(\"Falta fazer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradiente descendente estocástico\n",
    "\n",
    "Nos casos em que $N$ é um número grande, computar $\\nabla_{\\mathbf{w}}J(\\mathbf{w})$ a cada iteração se torna algo muito custoso. Uma estratégia para lidar com isso é **aproximar** $\\nabla_{\\mathbf{w}}J(\\mathbf{w})$ usando o gradiente:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\nabla_{\\mathbf{w}}J(\\mathbf{w})} = \\nabla_{\\mathbf{w}}\\frac{1}{m}\\sum_{i=1}^{m} L(h(\\mathbf{x}_{i}; \\mathbf{w}), \\; y_{i})\n",
    "\\end{equation}\n",
    "\n",
    "em que $(\\mathbf{x}_{1}, y_{1}), \\dots ,(\\mathbf{x}_{m}, y_{m})$ é uma amostragem aleatória dos dados de treinamento. A  estocasticidade surge da escolha desses $m$ dados (para que $\\hat{\\nabla_{\\mathbf{w}}J(\\mathbf{w})}$ seja um estimador não enviesado de $\\nabla_{\\mathbf{w}}J(\\mathbf{w})$ nós amostramos os $m$ dados a cada iteração). Normalmente usamos o nome **gradiente descendente estocástico** (*stochastic gradient descent* ou *online gradient descent*) quando $m=1$, e usamos o nome **minibatch stochastic gradient descent** quando $1 < m <N$ (nesse caso estamos usando apenas um pequeno lote dos dados, um *minibatch*). Usamos *batch* para referir a um *minibatch*, não confunda isso com *batch gradient descent*.\n",
    "\n",
    "\n",
    "**Stochastic gradient descent (SGD)**\n",
    "\n",
    "- $\\mathbf{w}(0) = \\mathbf{w}$\n",
    "- for $t = 0, 1, 2, \\dots$ do\n",
    "    * Sample a minibatch of $m$ examples from the training data.\n",
    "    * Compute the gradient estimate $\\hat{\\nabla_{\\mathbf{w}(t)}J(\\mathbf{w}(t))}$\n",
    "    * Apply update : $\\mathbf{w}(t+1) = \\mathbf{w}(t) - \\eta \\hat{\\nabla_{\\mathbf{w}(t)}J(\\mathbf{w}(t))}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  **Exercício 5)** \n",
    "Implemente o algoritmo stochastic gradient descent para a regressão linear com a taxa de apreendizado fixa. A saída da função é a mesma da função do exercício 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(X, y, w, learning_rate, num_iters, batch_size):\n",
    "    \"\"\"\n",
    "     Performs stochastic gradient descent optimization\n",
    "\n",
    "    :param X: design matrix\n",
    "    :type X: np.ndarray(shape=(N, d))\n",
    "    :param y: regression targets\n",
    "    :type y: np.ndarray(shape=(N, 1))\n",
    "    :param w: weights\n",
    "    :type w: np.array(shape=(d, 1))\n",
    "    :param learning_rate: learning rate\n",
    "    :type learning_rate: float\n",
    "    :param num_iters: number of iterations\n",
    "    :type num_iters: int\n",
    "    :param batch_size: size of the minibatch\n",
    "    :type batch_size: int\n",
    "    :return: weights, weights history, cost history\n",
    "    :rtype: np.array(shape=(d, 1)), list, list\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE:\n",
    "    raise NotImplementedError\n",
    "    # END YOUR CODE\n",
    "\n",
    "    return w, weights_history, cost_history "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Teste exercício 5)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    init = time.time()\n",
    "    learning_rate = 0.8\n",
    "    iterations = 2000\n",
    "    batch_size = 36\n",
    "    w, weights_history, cost_history = stochastic_gradient_descent(train_X_1,\n",
    "                                                                   train_y_norm,\n",
    "                                                                   initial_w,\n",
    "                                                                   learning_rate,\n",
    "                                                                   iterations,\n",
    "                                                                   batch_size)\n",
    "    assert cost_history[-1] < cost_history[0]\n",
    "    assert type(w) == np.ndarray\n",
    "    assert len(weights_history) == len(cost_history)\n",
    "    init = time.time() - init\n",
    "    print(\"Tempo de treinamento = {:.8f}(s)\".format(init))\n",
    "    print(\"Tem que ser em menos de 1.2 segundos\")\n",
    "    \n",
    "except (NotImplementedError, NameError):\n",
    "    print(\"Falta fazer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Podemos experimentar com diferentes tamanhos de batch para ver que quanto maior o tamanho do batch (mais próximo de $N$) menor a variância."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:    \n",
    "    hyper_params = [(0.001, 1000, 1),\n",
    "                    (0.001, 1000, 10),\n",
    "                    (0.001, 1000, 36)]\n",
    "    all_costs = []\n",
    "\n",
    "    for param in hyper_params:\n",
    "        learning_rate = param[0]\n",
    "        iterations = param[1]\n",
    "        batch_size = param[2]\n",
    "        _, weights_history, cost_history = stochastic_gradient_descent(train_X_1,\n",
    "                                                                       train_y_norm,\n",
    "                                                                       initial_w,\n",
    "                                                                       learning_rate,\n",
    "                                                                       iterations,\n",
    "                                                                       batch_size)\n",
    "        all_costs.append(cost_history)\n",
    "        title = \"Optimization landscape\\n\"\n",
    "        title += \"learning rate = {}\".format(learning_rate)\n",
    "        title += \" | iterations = {}\".format(iterations)\n",
    "        title += \" | batch size = {}\".format(batch_size)\n",
    "        plot_cost_function_curve(train_X_1,\n",
    "                                 train_y_norm,\n",
    "                                 compute_cost,\n",
    "                                 title=title,\n",
    "                                 weights_list=weights_history,\n",
    "                                 cost_list=cost_history)\n",
    "\n",
    "\n",
    "    _, _, cost_history_full = batch_gradient_descent(train_X_1,\n",
    "                                                     train_y_norm,\n",
    "                                                     initial_w,\n",
    "                                                     learning_rate=0.001,\n",
    "                                                     num_iters=1000)\n",
    "\n",
    "    all_costs.append(cost_history_full)\n",
    "    labels_size = [\"batch size = \" + str(param[2]) for param in hyper_params]\n",
    "    labels_size += [\"batch size = \" + str(train_X_1.shape[0])]\n",
    "\n",
    "    simple_step_plot(all_costs,\n",
    "                     \"loss\",\n",
    "                     'Training loss',\n",
    "                      figsize=(8, 8),\n",
    "                      labels=labels_size)\n",
    "\n",
    "\n",
    "except (NotImplementedError, NameError):\n",
    "    print(\"Falta fazer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Por que normalizar?\n",
    "\n",
    "O primeiro motivo para se normalizar os dados é para evitar *overflow*. Também é verdade que quando não normalizamos os dados as *features* podem apresentar diferentes escalas -- note que esse é o caso nesse dataset em que uma *feature* só tem $1$s e a outra ($m^{2}$) apresenta bastante variação. Isso influencia no gradiente de modo que a cada atualização os valores dos pesos vão mudar de modo diferente mesmo usando o mesmo *learning rate*.\n",
    "\n",
    "Isso pode ser visto quando acompanhamos a mudança nos pesos ao longo do treinamento no dataset original e no normalizado. Note como o parâmetro $\\mathbf{w}[1]$ (que pondera a feature ($m^{2}$)) muda bem mais que o parâmetro $\\mathbf{w}[0]$ quando usamos o dataset não normalizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:    \n",
    "    _, weights_history_norm, cost_history_norm = batch_gradient_descent(train_X_1,\n",
    "                                                                        train_y_norm,\n",
    "                                                                        initial_w,\n",
    "                                                                        learning_rate,\n",
    "                                                                        10)\n",
    "\n",
    "    train_X_1_non_norm = add_feature_ones(train_X)\n",
    "\n",
    "    w, weights_history, cost_history = batch_gradient_descent(train_X_1_non_norm,\n",
    "                                                              train_y,\n",
    "                                                              initial_w,\n",
    "                                                              0.000002,\n",
    "                                                              10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    w0_hist_norm = [w[0] for w in weights_history_norm]\n",
    "    w1_hist_norm = [w[1] for w in weights_history_norm]\n",
    "\n",
    "\n",
    "    w0mean, w0sdt, w0max, w0min = np.mean(w0_hist_norm), np.std(w0_hist_norm), np.max(w0_hist_norm), np.min(w0_hist_norm)\n",
    "    w1mean, w1sdt, w1max, w1min = np.mean(w0_hist_norm), np.std(w1_hist_norm), np.max(w1_hist_norm), np.min(w1_hist_norm)\n",
    "\n",
    "    print(\"\\nVariação dos pesos com o dataset normalizado\\n\")\n",
    "    print(\"w[0]:\\nmean {}, std {:.2f}, max {}, min {}\".format(w0mean, w0sdt, w0max, w0min))\n",
    "    print(\"w[1]:\\nmean {}, *std {:.2f}*, max {}, min {}\".format(w1mean, w1sdt, w1max, w1min))              \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    w0_hist = [w[0] for w in weights_history]\n",
    "    w1_hist = [w[1] for w in weights_history]\n",
    "\n",
    "\n",
    "    w0mean, w0sdt, w0max, w0min = np.mean(w0_hist), np.std(w0_hist), np.max(w0_hist), np.min(w0_hist)\n",
    "    w1mean, w1sdt, w1max, w1min = np.mean(w1_hist), np.std(w1_hist), np.max(w1_hist), np.min(w1_hist)\n",
    "\n",
    "    print(\"\\nVariação dos pesos com o dataset não normalizado\\n\")\n",
    "    print(\"w[0]:\\nmean {}, std {:.2f}, max {}, min {}\".format(w0mean, w0sdt, w0max, w0min))\n",
    "    print(\"w[1]:\\nmean {}, *std {:.2f}*, max {}, min {}\".format(w1mean, w1sdt, w1max, w1min))\n",
    "\n",
    "except (NotImplementedError, NameError):\n",
    "    print(\"Falta fazer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Um dos resultados dessa atualização em scala diferente para cada feature é a não convergência do algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:   \n",
    "    simple_step_plot([cost_history_norm],\n",
    "                     \"loss\",\n",
    "                     'Training loss (normalized)')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    simple_step_plot([cost_history],\n",
    "                     \"loss\",\n",
    "                     'Training loss (non normalized)')\n",
    "\n",
    "\n",
    "\n",
    "    plot_cost_function_curve(train_X_1_non_norm,\n",
    "                             train_y,\n",
    "                             compute_cost,\n",
    "                             title=\"Optimization landscape\\n(non normalized data)\",\n",
    "                             weights_list=weights_history,\n",
    "                             cost_list=cost_history,\n",
    "                             range_points=(100, 100))\n",
    "except (NotImplementedError, NameError):\n",
    "    print(\"Falta fazer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mais otimização!\n",
    "\n",
    "Há muitos outros algoritmos de otimização construídos em cima da ideia de gradiente descendente. Um bom resumo de alguns desses algoritimos pode ser encontrado [aqui](http://ruder.io/optimizing-gradient-descent/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
