{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = '180612_v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# all imports\n",
    "import sys\n",
    "sys.path.insert(0, '../../src/')\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import transformers\n",
    "import numpy as np\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import make_union\n",
    "from transformers import (ModelTransformer, DataFrameColumnExtractor, ToDictTransformer)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import cross_validation as cv\n",
    "from sklearn import metrics\n",
    "\n",
    "#evaluators\n",
    "#gridcv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../../data/raw/'\n",
    "TRAIN_DATASET_PATH = os.path.join(DATA_DIR, 'train.csv')\n",
    "TEST_DATASET_PATH = os.path.join(DATA_DIR, 'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(TRAIN_DATASET_PATH, encoding='utf-8')\n",
    "df_test = pd.read_csv(TEST_DATASET_PATH, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.drop(['y', 'id'], axis=1)\n",
    "y = df_train['y']#.astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = cv.train_test_split(X,\n",
    "                                                       y,\n",
    "                                                       test_size=0.25,\n",
    "                                                       random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test = df_test.drop(['id'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "bin numeric?\n",
    "\n",
    "pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n",
    "\n",
    "skewed: campaign, pdays, previous, emp.var.rate, euribor3m, nr.employed\n",
    "\n",
    "unbalanced data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL_FEATURES = [\n",
    "    'job',\n",
    "    'marital',\n",
    "    'education',\n",
    "    'default',\n",
    "    'housing',\n",
    "    'loan',\n",
    "    'contact',\n",
    "    'poutcome',\n",
    "    'month',\n",
    "    'day_of_week'\n",
    "]\n",
    "\n",
    "NUMERIC_FEATURES = [                    \n",
    "    'age',\n",
    "    'campaign',\n",
    "    'pdays',\n",
    "    'previous',\n",
    "    'emp.var.rate',\n",
    "    'cons.price.idx',\n",
    "    'cons.conf.idx',\n",
    "    'euribor3m',\n",
    "    'nr.employed'\n",
    "]\n",
    "\n",
    "TO_APPLY_LOG = [\n",
    "]\n",
    "\n",
    "TO_APPLY_CUBE_ROOT = [\n",
    "\n",
    "]\n",
    "\n",
    "TO_APPLY_RECIPROCAL = [\n",
    "]\n",
    "\n",
    "TO_BIN = [\n",
    "\n",
    "]\n",
    "\n",
    "TRAINING_FEATURES = NUMERIC_FEATURES + CATEGORICAL_FEATURES\n",
    "ALL_COLUMNS = TRAINING_FEATURES + ['id', 'y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_pipeline = make_pipeline(\n",
    "    make_union(\n",
    "        make_pipeline(\n",
    "            make_union(\n",
    "                make_pipeline(\n",
    "                    DataFrameColumnExtractor(NUMERIC_FEATURES),\n",
    "#                     Imputer(strategy=\"median\", axis=0),\n",
    "                )\n",
    "            ),\n",
    "            StandardScaler(),\n",
    "        ),\n",
    "        make_pipeline(\n",
    "            DataFrameColumnExtractor(CATEGORICAL_FEATURES),\n",
    "            ToDictTransformer(),\n",
    "            DictVectorizer(sparse=False)\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_pipeline = make_pipeline(\n",
    "#      make_union(\n",
    "#          ModelTransformer(LogisticRegression(random_state=42)),\n",
    "#          FunctionTransformer(transformers.all_columns)\n",
    "#      ),\n",
    "     GradientBoostingClassifier(random_state=42, max_depth=4) \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline= Pipeline([\n",
    "    ('features', transformer_pipeline),\n",
    "    ('predictor', predictor_pipeline)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('features', Pipeline(steps=[('featureunion', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('pipeline-1', Pipeline(steps=[('featureunion', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('pipeline', Pipeline(steps=[('dataframecolumnextractor', DataFrameColumnExtractor(columns=['age', 'cam...100, presort='auto', random_state=42,\n",
       "              subsample=1.0, verbose=0, warm_start=False))]))])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_test)\n",
    "y_pred_proba = pipeline.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.98      0.95      8077\n",
      "          1       0.67      0.26      0.37       985\n",
      "\n",
      "avg / total       0.89      0.91      0.89      9062\n",
      "\n",
      "Precision: 0.6737967914438503\n",
      "Recall: 0.25583756345177666\n",
      "F1 Score: 0.3708609271523179\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Precision: \"+ str(metrics.precision_score(y_test, y_pred)))\n",
    "print(\"Recall: \"+ str(metrics.recall_score(y_test, y_pred)))\n",
    "print(\"F1 Score: \"+ str(metrics.f1_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../../data/raw/'\n",
    "TRAIN_DATASET_PATH = os.path.join(DATA_DIR, 'train.csv')\n",
    "TEST_DATASET_PATH = os.path.join(DATA_DIR, 'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(TRAIN_DATASET_PATH, encoding='utf-8')\n",
    "df_test = pd.read_csv(TEST_DATASET_PATH, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.drop(['y', 'id'], axis=1)\n",
    "y = df_train['y']#.astype(bool)\n",
    "\n",
    "X_test = df_test.drop(['id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('features', Pipeline(steps=[('featureunion', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('pipeline-1', Pipeline(steps=[('featureunion', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('pipeline', Pipeline(steps=[('dataframecolumnextractor', DataFrameColumnExtractor(columns=['age', 'cam...100, presort='auto', random_state=42,\n",
       "              subsample=1.0, verbose=0, warm_start=False))]))])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_submission = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_submission = df_test['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save submission file\n",
    "SUBMISSION_DATA_DIR = '../../data/submission/'\n",
    "SUBMISSION_FILE_PATH = os.path.join(SUBMISSION_DATA_DIR, VERSION+'.csv')\n",
    "\n",
    "df_submission = pd.DataFrame({'id':ids_submission, 'y':pred_submission})\n",
    "df_submission.to_csv(SUBMISSION_FILE_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9062 entries, 0 to 9061\n",
      "Data columns (total 2 columns):\n",
      "id    9062 non-null int64\n",
      "y     9062 non-null int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 141.7 KB\n"
     ]
    }
   ],
   "source": [
    "df_submission.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
