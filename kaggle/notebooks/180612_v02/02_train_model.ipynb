{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ToDo:\n",
    "    * Create new feature (999)\n",
    "    * 1.11.5.3. Using the VotingClassifier with GridSearch\n",
    "    * bin\n",
    "    * other cat encoding\n",
    "    * cross valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done:\n",
    "    * Double pos (not improve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changelog:\n",
    "    * maj vot\n",
    "    * use reciprocal\n",
    "    * use kbest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = '180612_v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all imports\n",
    "import sys\n",
    "sys.path.insert(0, '../../src/')\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import transformers\n",
    "import numpy as np\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import make_union\n",
    "from transformers import (ModelTransformer, DataFrameColumnExtractor, ToDictTransformer)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import cross_validation as cv\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "#evaluators\n",
    "#gridcv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../../data/raw/'\n",
    "TRAIN_DATASET_PATH = os.path.join(DATA_DIR, 'train.csv')\n",
    "TEST_DATASET_PATH = os.path.join(DATA_DIR, 'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(TRAIN_DATASET_PATH, encoding='utf-8')\n",
    "df_test = pd.read_csv(TEST_DATASET_PATH, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.drop(['y', 'id'], axis=1)\n",
    "y = df_train['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = cv.train_test_split(X,\n",
    "                                                       y,\n",
    "                                                       test_size=0.25,\n",
    "                                                       random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_yes = X_train[y_train == 1]\n",
    "# y_train_yes = y_train[y_train == 1]\n",
    "\n",
    "# X_train = pd.concat([X_train, X_train_yes])\n",
    "# y_train = pd.concat([y_train, y_train_yes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                34906\n",
       "y                 34906\n",
       "age               34906\n",
       "job               34906\n",
       "marital           34906\n",
       "education         34906\n",
       "default           34906\n",
       "housing           34906\n",
       "loan              34906\n",
       "contact           34906\n",
       "month             34906\n",
       "day_of_week       34906\n",
       "campaign          34906\n",
       "pdays             34906\n",
       "previous          34906\n",
       "poutcome          34906\n",
       "emp.var.rate      34906\n",
       "cons.price.idx    34906\n",
       "cons.conf.idx     34906\n",
       "euribor3m         34906\n",
       "nr.employed       34906\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[df_train['pdays'] == 999].count()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "bin numeric?\n",
    "\n",
    "pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n",
    "\n",
    "skewed: campaign, pdays, previous, emp.var.rate, euribor3m, nr.employed\n",
    "\n",
    "unbalanced data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL_FEATURES = [\n",
    "    'job',\n",
    "    'marital',\n",
    "    'education',\n",
    "    'default',\n",
    "    'housing',\n",
    "    'loan',\n",
    "    'contact',\n",
    "    'poutcome',\n",
    "    'month',\n",
    "    'day_of_week'\n",
    "]\n",
    "\n",
    "NUMERIC_FEATURES = [                    \n",
    "    'age',\n",
    "#     'campaign',\n",
    "    'pdays',\n",
    "#     'previous',\n",
    "    'emp.var.rate',\n",
    "    'cons.price.idx',\n",
    "    'cons.conf.idx',\n",
    "    'euribor3m',\n",
    "    'nr.employed'\n",
    "]\n",
    "\n",
    "TO_APPLY_LOG = [\n",
    "]\n",
    "\n",
    "TO_APPLY_CUBE_ROOT = [\n",
    "\n",
    "]\n",
    "\n",
    "TO_APPLY_RECIPROCAL = [\n",
    "    'campaign',\n",
    "    'previous'\n",
    "]\n",
    "\n",
    "TO_BIN = [\n",
    "\n",
    "]\n",
    "\n",
    "TRAINING_FEATURES = NUMERIC_FEATURES + CATEGORICAL_FEATURES\n",
    "ALL_COLUMNS = TRAINING_FEATURES + ['id', 'y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_pipeline = make_pipeline(\n",
    "    make_pipeline(\n",
    "\n",
    "        make_union(\n",
    "            make_pipeline(\n",
    "                make_union(\n",
    "                    make_pipeline(\n",
    "                        DataFrameColumnExtractor(NUMERIC_FEATURES),\n",
    "                    ),\n",
    "                    make_pipeline(\n",
    "                        DataFrameColumnExtractor(TO_APPLY_RECIPROCAL),\n",
    "                        FunctionTransformer(transformers.sum_1),\n",
    "                        FunctionTransformer(transformers.apply_reciprocal),\n",
    "                    )\n",
    "                ),\n",
    "                StandardScaler(),\n",
    "            ),\n",
    "            make_pipeline(\n",
    "                DataFrameColumnExtractor(CATEGORICAL_FEATURES),\n",
    "                ToDictTransformer(),\n",
    "                DictVectorizer(sparse=False)\n",
    "            ),\n",
    "\n",
    "        ),\n",
    "        SelectKBest(f_classif, k=55)\n",
    "    )\n",
    ")\n",
    "\n",
    "# DataFrameColumnExtractor(to_bin),\n",
    "# FunctionTransformer(transformers.bin_data, kw_args={'columns': to_bin, 'bins': bins}),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = VotingClassifier(estimators=[\n",
    "#     ('gbc', GradientBoostingClassifier(random_state=42, max_depth=4)), \n",
    "#     ('xgb', XGBClassifier(random_state=42)), \n",
    "#     ('lr', LogisticRegression(random_state=42)),\n",
    "#     ('rf', RandomForestClassifier(random_state=42)),\n",
    "#     ('gau', GaussianNB())\n",
    "# ], voting='soft')\n",
    "\n",
    "predictor_pipeline = make_pipeline(\n",
    "    VotingClassifier(estimators=[\n",
    "        ('gbc', GradientBoostingClassifier(random_state=42, max_depth=4)), \n",
    "        ('xgb', XGBClassifier(random_state=42)), \n",
    "        ('lr', LogisticRegression(random_state=42)),\n",
    "        ('rf', RandomForestClassifier(random_state=42)),\n",
    "        ('gau', GaussianNB()),\n",
    "        ('nn', MLPClassifier(random_state=42, solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2))),\n",
    "#         ('svm', SVC(random_state=42, probability=True)),\n",
    "    ], voting='soft')\n",
    ")\n",
    "\n",
    "#      make_union(\n",
    "#          ModelTransformer(SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "#             decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
    "#             max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "#             tol=0.001, verbose=False)),\n",
    "#          FunctionTransformer(transformers.all_columns)\n",
    "#      ),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline= Pipeline([\n",
    "    ('features', transformer_pipeline),\n",
    "    ('predictor', predictor_pipeline)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/__init__.py:54: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(mask.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('features', Pipeline(steps=[('pipeline', Pipeline(steps=[('featureunion', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('pipeline-1', Pipeline(steps=[('featureunion', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('pipeline-1', Pipeline(steps=[('dataframecolumnextractor', DataFrameColum...0.1, verbose=False,\n",
       "       warm_start=False))],\n",
       "         n_jobs=1, voting='soft', weights=None))]))])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/__init__.py:54: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(mask.dtype, np.int):\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/label.py:171: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "y_pred = pipeline.predict(X_test)\n",
    "y_pred_proba = pipeline.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.97      0.95      8077\n",
      "          1       0.61      0.35      0.45       985\n",
      "\n",
      "avg / total       0.89      0.91      0.89      9062\n",
      "\n",
      "Precision: 0.6141592920353982\n",
      "Recall: 0.3522842639593909\n",
      "F1 Score: 0.8939386128529604\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Precision: \"+ str(metrics.precision_score(y_test, y_pred)))\n",
    "print(\"Recall: \"+ str(metrics.recall_score(y_test, y_pred)))\n",
    "print(\"F1 Score: \"+ str(metrics.f1_score(y_test, y_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../../data/raw/'\n",
    "TRAIN_DATASET_PATH = os.path.join(DATA_DIR, 'train.csv')\n",
    "TEST_DATASET_PATH = os.path.join(DATA_DIR, 'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(TRAIN_DATASET_PATH, encoding='utf-8')\n",
    "df_test = pd.read_csv(TEST_DATASET_PATH, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.drop(['y', 'id'], axis=1)\n",
    "y_train = df_train['y']\n",
    "\n",
    "X_test = df_test.drop(['id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_yes = X_train[y_train == 1]\n",
    "# y_train_yes = y_train[y_train == 1]\n",
    "\n",
    "# X_train = pd.concat([X_train, X_train_yes])\n",
    "# y_train = pd.concat([y_train, y_train_yes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/__init__.py:54: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(mask.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('features', Pipeline(steps=[('pipeline', Pipeline(steps=[('featureunion', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('pipeline-1', Pipeline(steps=[('featureunion', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('pipeline-1', Pipeline(steps=[('dataframecolumnextractor', DataFrameColum...0.1, verbose=False,\n",
       "       warm_start=False))],\n",
       "         n_jobs=1, voting='soft', weights=None))]))])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/__init__.py:54: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(mask.dtype, np.int):\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/label.py:171: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "pred_submission = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_submission = df_test['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save submission file\n",
    "SUBMISSION_DATA_DIR = '../../data/submission/'\n",
    "SUBMISSION_FILE_PATH = os.path.join(SUBMISSION_DATA_DIR, VERSION+'.csv')\n",
    "\n",
    "df_submission = pd.DataFrame({'id':ids_submission, 'y':pred_submission})\n",
    "df_submission.to_csv(SUBMISSION_FILE_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9062 entries, 0 to 9061\n",
      "Data columns (total 2 columns):\n",
      "id    9062 non-null int64\n",
      "y     9062 non-null int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 141.7 KB\n"
     ]
    }
   ],
   "source": [
    "df_submission.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
